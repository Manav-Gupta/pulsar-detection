{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pulsar-star-detection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUQbL1Xp5mrIQzTPG+WiES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manav-Gupta/pulsar-detection/blob/main/pulsar_star_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "959Ih8xYWb8y",
        "outputId": "d05dc2be-0dc8-4cce-f1ca-8ffd214aee65"
      },
      "source": [
        "# mounting the drive to access the dataset\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNxYoDGvWwTp"
      },
      "source": [
        "# importing all the required libraries\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from math import sqrt\r\n",
        "import keras\r\n",
        "from pandas import DataFrame\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.optimizers import Adam\r\n",
        "from datetime import datetime\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "4iOrTPPFW5T9",
        "outputId": "7e1475f7-cead-4995-c4a2-cf93b4d2cf67"
      },
      "source": [
        "# reading the dataset\r\n",
        "df=pd.read_csv('drive/My Drive/HTRU_2.csv')\r\n",
        "df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>140.5625</th>\n",
              "      <th>55.68378214</th>\n",
              "      <th>-0.234571412</th>\n",
              "      <th>-0.699648398</th>\n",
              "      <th>3.199832776</th>\n",
              "      <th>19.11042633</th>\n",
              "      <th>7.975531794</th>\n",
              "      <th>74.24222492</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102.507812</td>\n",
              "      <td>58.882430</td>\n",
              "      <td>0.465318</td>\n",
              "      <td>-0.515088</td>\n",
              "      <td>1.677258</td>\n",
              "      <td>14.860146</td>\n",
              "      <td>10.576487</td>\n",
              "      <td>127.393580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>103.015625</td>\n",
              "      <td>39.341649</td>\n",
              "      <td>0.323328</td>\n",
              "      <td>1.051164</td>\n",
              "      <td>3.121237</td>\n",
              "      <td>21.744669</td>\n",
              "      <td>7.735822</td>\n",
              "      <td>63.171909</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>136.750000</td>\n",
              "      <td>57.178449</td>\n",
              "      <td>-0.068415</td>\n",
              "      <td>-0.636238</td>\n",
              "      <td>3.642977</td>\n",
              "      <td>20.959280</td>\n",
              "      <td>6.896499</td>\n",
              "      <td>53.593661</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>88.726562</td>\n",
              "      <td>40.672225</td>\n",
              "      <td>0.600866</td>\n",
              "      <td>1.123492</td>\n",
              "      <td>1.178930</td>\n",
              "      <td>11.468720</td>\n",
              "      <td>14.269573</td>\n",
              "      <td>252.567306</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>93.570312</td>\n",
              "      <td>46.698114</td>\n",
              "      <td>0.531905</td>\n",
              "      <td>0.416721</td>\n",
              "      <td>1.636288</td>\n",
              "      <td>14.545074</td>\n",
              "      <td>10.621748</td>\n",
              "      <td>131.394004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     140.5625  55.68378214  -0.234571412  ...  7.975531794  74.24222492  0\n",
              "0  102.507812    58.882430      0.465318  ...    10.576487   127.393580  0\n",
              "1  103.015625    39.341649      0.323328  ...     7.735822    63.171909  0\n",
              "2  136.750000    57.178449     -0.068415  ...     6.896499    53.593661  0\n",
              "3   88.726562    40.672225      0.600866  ...    14.269573   252.567306  0\n",
              "4   93.570312    46.698114      0.531905  ...    10.621748   131.394004  0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wORSncf8a90m"
      },
      "source": [
        "# renaming the columns\r\n",
        "df.columns=[' Mean of the integrated profile',' Standard deviation of the integrated profile',' Excess kurtosis of the integrated profile',' Skewness of the integrated profile',' Mean of the DM-SNR curve',' Standard deviation of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve',' Skewness of the DM-SNR curve','target_class']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "G-A6CjEGbRZA",
        "outputId": "56b777e1-1b78-4ed4-8fa2-e41de26e8058"
      },
      "source": [
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean of the integrated profile</th>\n",
              "      <th>Standard deviation of the integrated profile</th>\n",
              "      <th>Excess kurtosis of the integrated profile</th>\n",
              "      <th>Skewness of the integrated profile</th>\n",
              "      <th>Mean of the DM-SNR curve</th>\n",
              "      <th>Standard deviation of the DM-SNR curve</th>\n",
              "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
              "      <th>Skewness of the DM-SNR curve</th>\n",
              "      <th>target_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102.507812</td>\n",
              "      <td>58.882430</td>\n",
              "      <td>0.465318</td>\n",
              "      <td>-0.515088</td>\n",
              "      <td>1.677258</td>\n",
              "      <td>14.860146</td>\n",
              "      <td>10.576487</td>\n",
              "      <td>127.393580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>103.015625</td>\n",
              "      <td>39.341649</td>\n",
              "      <td>0.323328</td>\n",
              "      <td>1.051164</td>\n",
              "      <td>3.121237</td>\n",
              "      <td>21.744669</td>\n",
              "      <td>7.735822</td>\n",
              "      <td>63.171909</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>136.750000</td>\n",
              "      <td>57.178449</td>\n",
              "      <td>-0.068415</td>\n",
              "      <td>-0.636238</td>\n",
              "      <td>3.642977</td>\n",
              "      <td>20.959280</td>\n",
              "      <td>6.896499</td>\n",
              "      <td>53.593661</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>88.726562</td>\n",
              "      <td>40.672225</td>\n",
              "      <td>0.600866</td>\n",
              "      <td>1.123492</td>\n",
              "      <td>1.178930</td>\n",
              "      <td>11.468720</td>\n",
              "      <td>14.269573</td>\n",
              "      <td>252.567306</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>93.570312</td>\n",
              "      <td>46.698114</td>\n",
              "      <td>0.531905</td>\n",
              "      <td>0.416721</td>\n",
              "      <td>1.636288</td>\n",
              "      <td>14.545074</td>\n",
              "      <td>10.621748</td>\n",
              "      <td>131.394004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17892</th>\n",
              "      <td>136.429688</td>\n",
              "      <td>59.847421</td>\n",
              "      <td>-0.187846</td>\n",
              "      <td>-0.738123</td>\n",
              "      <td>1.296823</td>\n",
              "      <td>12.166062</td>\n",
              "      <td>15.450260</td>\n",
              "      <td>285.931022</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17893</th>\n",
              "      <td>122.554688</td>\n",
              "      <td>49.485605</td>\n",
              "      <td>0.127978</td>\n",
              "      <td>0.323061</td>\n",
              "      <td>16.409699</td>\n",
              "      <td>44.626893</td>\n",
              "      <td>2.945244</td>\n",
              "      <td>8.297092</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17894</th>\n",
              "      <td>119.335938</td>\n",
              "      <td>59.935939</td>\n",
              "      <td>0.159363</td>\n",
              "      <td>-0.743025</td>\n",
              "      <td>21.430602</td>\n",
              "      <td>58.872000</td>\n",
              "      <td>2.499517</td>\n",
              "      <td>4.595173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17895</th>\n",
              "      <td>114.507812</td>\n",
              "      <td>53.902400</td>\n",
              "      <td>0.201161</td>\n",
              "      <td>-0.024789</td>\n",
              "      <td>1.946488</td>\n",
              "      <td>13.381731</td>\n",
              "      <td>10.007967</td>\n",
              "      <td>134.238910</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17896</th>\n",
              "      <td>57.062500</td>\n",
              "      <td>85.797340</td>\n",
              "      <td>1.406391</td>\n",
              "      <td>0.089520</td>\n",
              "      <td>188.306020</td>\n",
              "      <td>64.712562</td>\n",
              "      <td>-1.597527</td>\n",
              "      <td>1.429475</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17897 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Mean of the integrated profile  ...  target_class\n",
              "0                           102.507812  ...             0\n",
              "1                           103.015625  ...             0\n",
              "2                           136.750000  ...             0\n",
              "3                            88.726562  ...             0\n",
              "4                            93.570312  ...             0\n",
              "...                                ...  ...           ...\n",
              "17892                       136.429688  ...             0\n",
              "17893                       122.554688  ...             0\n",
              "17894                       119.335938  ...             0\n",
              "17895                       114.507812  ...             0\n",
              "17896                        57.062500  ...             0\n",
              "\n",
              "[17897 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "rCsZRu6hbe70",
        "outputId": "d37640b0-047c-4d50-fa4c-fb1d0fd96f67"
      },
      "source": [
        "df=df[[' Mean of the integrated profile',' Standard deviation of the integrated profile',' Excess kurtosis of the integrated profile',' Skewness of the integrated profile',' Mean of the DM-SNR curve',' Standard deviation of the DM-SNR curve',' Excess kurtosis of the DM-SNR curve',' Skewness of the DM-SNR curve','target_class']]\r\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean of the integrated profile</th>\n",
              "      <th>Standard deviation of the integrated profile</th>\n",
              "      <th>Excess kurtosis of the integrated profile</th>\n",
              "      <th>Skewness of the integrated profile</th>\n",
              "      <th>Mean of the DM-SNR curve</th>\n",
              "      <th>Standard deviation of the DM-SNR curve</th>\n",
              "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
              "      <th>Skewness of the DM-SNR curve</th>\n",
              "      <th>target_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>102.507812</td>\n",
              "      <td>58.882430</td>\n",
              "      <td>0.465318</td>\n",
              "      <td>-0.515088</td>\n",
              "      <td>1.677258</td>\n",
              "      <td>14.860146</td>\n",
              "      <td>10.576487</td>\n",
              "      <td>127.393580</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>103.015625</td>\n",
              "      <td>39.341649</td>\n",
              "      <td>0.323328</td>\n",
              "      <td>1.051164</td>\n",
              "      <td>3.121237</td>\n",
              "      <td>21.744669</td>\n",
              "      <td>7.735822</td>\n",
              "      <td>63.171909</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>136.750000</td>\n",
              "      <td>57.178449</td>\n",
              "      <td>-0.068415</td>\n",
              "      <td>-0.636238</td>\n",
              "      <td>3.642977</td>\n",
              "      <td>20.959280</td>\n",
              "      <td>6.896499</td>\n",
              "      <td>53.593661</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>88.726562</td>\n",
              "      <td>40.672225</td>\n",
              "      <td>0.600866</td>\n",
              "      <td>1.123492</td>\n",
              "      <td>1.178930</td>\n",
              "      <td>11.468720</td>\n",
              "      <td>14.269573</td>\n",
              "      <td>252.567306</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>93.570312</td>\n",
              "      <td>46.698114</td>\n",
              "      <td>0.531905</td>\n",
              "      <td>0.416721</td>\n",
              "      <td>1.636288</td>\n",
              "      <td>14.545074</td>\n",
              "      <td>10.621748</td>\n",
              "      <td>131.394004</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17892</th>\n",
              "      <td>136.429688</td>\n",
              "      <td>59.847421</td>\n",
              "      <td>-0.187846</td>\n",
              "      <td>-0.738123</td>\n",
              "      <td>1.296823</td>\n",
              "      <td>12.166062</td>\n",
              "      <td>15.450260</td>\n",
              "      <td>285.931022</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17893</th>\n",
              "      <td>122.554688</td>\n",
              "      <td>49.485605</td>\n",
              "      <td>0.127978</td>\n",
              "      <td>0.323061</td>\n",
              "      <td>16.409699</td>\n",
              "      <td>44.626893</td>\n",
              "      <td>2.945244</td>\n",
              "      <td>8.297092</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17894</th>\n",
              "      <td>119.335938</td>\n",
              "      <td>59.935939</td>\n",
              "      <td>0.159363</td>\n",
              "      <td>-0.743025</td>\n",
              "      <td>21.430602</td>\n",
              "      <td>58.872000</td>\n",
              "      <td>2.499517</td>\n",
              "      <td>4.595173</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17895</th>\n",
              "      <td>114.507812</td>\n",
              "      <td>53.902400</td>\n",
              "      <td>0.201161</td>\n",
              "      <td>-0.024789</td>\n",
              "      <td>1.946488</td>\n",
              "      <td>13.381731</td>\n",
              "      <td>10.007967</td>\n",
              "      <td>134.238910</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17896</th>\n",
              "      <td>57.062500</td>\n",
              "      <td>85.797340</td>\n",
              "      <td>1.406391</td>\n",
              "      <td>0.089520</td>\n",
              "      <td>188.306020</td>\n",
              "      <td>64.712562</td>\n",
              "      <td>-1.597527</td>\n",
              "      <td>1.429475</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17897 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Mean of the integrated profile  ...  target_class\n",
              "0                           102.507812  ...             0\n",
              "1                           103.015625  ...             0\n",
              "2                           136.750000  ...             0\n",
              "3                            88.726562  ...             0\n",
              "4                            93.570312  ...             0\n",
              "...                                ...  ...           ...\n",
              "17892                       136.429688  ...             0\n",
              "17893                       122.554688  ...             0\n",
              "17894                       119.335938  ...             0\n",
              "17895                       114.507812  ...             0\n",
              "17896                        57.062500  ...             0\n",
              "\n",
              "[17897 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shnJaClWeM0d"
      },
      "source": [
        "#Deleting the rows containing NaN values\r\n",
        "df=df.dropna()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIOqE6wfes3q"
      },
      "source": [
        "#Creating the feature test set\r\n",
        "df1= df.drop(['target_class'],axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MKYn4LFe1Lt"
      },
      "source": [
        "#Creating the target test set\r\n",
        "df2= df['target_class'].values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szgEPLg9e8li",
        "outputId": "d286a7f1-7835-41a5-fbf5-15a709430117"
      },
      "source": [
        "#Scaling data\r\n",
        "scale_x = MinMaxScaler(feature_range=(0,1))\r\n",
        "scaled_x=scale_x.fit_transform(df1)\r\n",
        "print(scaled_x.shape)\r\n",
        "df2=df2.reshape(-1, 1)\r\n",
        "scale_y = MinMaxScaler(feature_range=(0,1))\r\n",
        "scaled_y=scale_y.fit_transform(df2)\r\n",
        "print(scaled_y.shape)\r\n",
        "print(scaled_x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17897, 8)\n",
            "(17897, 1)\n",
            "[[0.51762787 0.46090841 0.23541516 ... 0.0725243  0.36401483 0.10844339]\n",
            " [0.52034628 0.19686832 0.22113842 ... 0.13918843 0.28862387 0.05461031]\n",
            " [0.70093263 0.43788377 0.18174959 ... 0.13158337 0.26634832 0.04658145]\n",
            " ...\n",
            " [0.60771193 0.4751437  0.2046521  ... 0.49869934 0.14965285 0.00550903]\n",
            " [0.58186609 0.39361695 0.20885482 ... 0.05820853 0.34892638 0.11418141]\n",
            " [0.27435072 0.82458965 0.33003783 ... 0.5552546  0.04091771 0.00285542]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPFpvru5e_IQ",
        "outputId": "9ee21be5-f99a-4ec0-e401-a6863c87bbf5"
      },
      "source": [
        "#Converting data into readable format of LSTM for previous 100 timesteps\r\n",
        "x=list()\r\n",
        "y=list()\r\n",
        "ws=100\r\n",
        "for i in range(len(df)-ws):\r\n",
        "  xx=list()\r\n",
        "  for j in range(0,ws-1,1):\r\n",
        "    value=scaled_x[i+j]\r\n",
        "    xx.append(value)\r\n",
        "  x.append(xx)\r\n",
        "  yy=scaled_y[i+ws-1]\r\n",
        "  y.append(yy)\r\n",
        "#Input of  LSTM \r\n",
        "x=np.array(x)\r\n",
        "#Output of LSTM\r\n",
        "y=np.array(y)\r\n",
        "print(x.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17797, 99, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QawsUGyGfBfk"
      },
      "source": [
        "#Splitting data into training 70% , testing 15% and validation 15%\r\n",
        "n_train_hours = 12493\r\n",
        "n_valid_hours = 15170\r\n",
        "train_x = x[:n_train_hours]\r\n",
        "test_x = x[n_valid_hours:]\r\n",
        "valid_x= x[n_train_hours:n_valid_hours]\r\n",
        "train_y = y[:n_train_hours]\r\n",
        "test_y = y[n_valid_hours:]\r\n",
        "valid_y= y[n_train_hours:n_valid_hours]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cblkyy7M6Wib",
        "outputId": "0edc365d-b01e-42ab-93c3-fac956d77e2f"
      },
      "source": [
        "#Declaration of model\r\n",
        "model = Sequential()\r\n",
        "#adding LSTM layer\r\n",
        "model.add(LSTM(64, input_shape=(9,8),kernel_initializer='truncated_normal'))\r\n",
        "model.add(Dense(1,activation='linear'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001))\r\n",
        "#Training the model\r\n",
        "history = model.fit(train_x, train_y, epochs=300, batch_size=1000, validation_data=(valid_x, valid_y), verbose=1, shuffle=False)\r\n",
        "#Plotting the loss graphs\r\n",
        "plt.plot(history.history['loss'], label='train')\r\n",
        "#plt.plot(history.history['val_loss'], label='valid')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 9, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 99, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 9, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 99, 8).\n",
            "13/13 [==============================] - ETA: 0s - loss: 1.4546e-08WARNING:tensorflow:Model was constructed with shape (None, 9, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9, 8), dtype=tf.float32, name='lstm_input'), name='lstm_input', description=\"created by layer 'lstm_input'\"), but it was called on an input with incompatible shape (None, 99, 8).\n",
            "13/13 [==============================] - 10s 585ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 2/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 3/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 4/300\n",
            "13/13 [==============================] - 7s 531ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 5/300\n",
            "13/13 [==============================] - 7s 529ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 6/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 7/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 8/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 9/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 10/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 11/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 12/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 13/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 14/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 15/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 16/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 17/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 18/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 19/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 20/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 21/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 22/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 23/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 24/300\n",
            "13/13 [==============================] - 7s 547ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 25/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 26/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 27/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 28/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 29/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 30/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 31/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 32/300\n",
            "13/13 [==============================] - 11s 905ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 33/300\n",
            "13/13 [==============================] - 8s 587ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 34/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 35/300\n",
            "13/13 [==============================] - 7s 547ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 36/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 37/300\n",
            "13/13 [==============================] - 7s 550ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 38/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 39/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 40/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 41/300\n",
            "13/13 [==============================] - 7s 548ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 42/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 43/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 44/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 45/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 46/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 47/300\n",
            "13/13 [==============================] - 7s 532ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 48/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 49/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 50/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 51/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 52/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 53/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 54/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 55/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 56/300\n",
            "13/13 [==============================] - 7s 550ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 57/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 58/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 59/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 60/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 61/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 62/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 63/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 64/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 65/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 66/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 67/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 68/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 69/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 70/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 71/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 72/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 73/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 74/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 75/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 76/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 77/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 78/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 79/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 80/300\n",
            "13/13 [==============================] - 7s 553ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 81/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 82/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 83/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 84/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 85/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 86/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 87/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 88/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 89/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 90/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 91/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 92/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 93/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 94/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 95/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 96/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 97/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 98/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 99/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 100/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 101/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 102/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 103/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 104/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 105/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 106/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 107/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 108/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 109/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 110/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 111/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 112/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 113/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 114/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 115/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 116/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 117/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 118/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 119/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 120/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 121/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 122/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 123/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 124/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 125/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 126/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 127/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 128/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 129/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 130/300\n",
            "13/13 [==============================] - 7s 532ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 131/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 132/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 133/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 134/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 135/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 136/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 137/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 138/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 139/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 140/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 141/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 142/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 143/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 144/300\n",
            "13/13 [==============================] - 7s 549ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 145/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 146/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 147/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 148/300\n",
            "13/13 [==============================] - 7s 548ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 149/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 150/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 151/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 152/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 153/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 154/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 155/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 156/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 157/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 158/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 159/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 160/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 161/300\n",
            "13/13 [==============================] - 7s 548ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 162/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 163/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 164/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 165/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 166/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 167/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 168/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 169/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 170/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 171/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 172/300\n",
            "13/13 [==============================] - 7s 529ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 173/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 174/300\n",
            "13/13 [==============================] - 7s 530ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 175/300\n",
            "13/13 [==============================] - 7s 530ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 176/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 177/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 178/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 179/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 180/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 181/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 182/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 183/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 184/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 185/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 186/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 187/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 188/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 189/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 190/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 191/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 192/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 193/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 194/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 195/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 196/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 197/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 198/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 199/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 200/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 201/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 202/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 203/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 204/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 205/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 206/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 207/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 208/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 209/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 210/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 211/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 212/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 213/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 214/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 215/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 216/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 217/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 218/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 219/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 220/300\n",
            "13/13 [==============================] - 7s 532ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 221/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 222/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 223/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 224/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 225/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 226/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 227/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 228/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 229/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 230/300\n",
            "13/13 [==============================] - 7s 531ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 231/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 232/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 233/300\n",
            "13/13 [==============================] - 7s 531ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 234/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 235/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 236/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 237/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 238/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 239/300\n",
            "13/13 [==============================] - 7s 529ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 240/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 241/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 242/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 243/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 244/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 245/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 246/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 247/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 248/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 249/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 250/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 251/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 252/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 253/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 254/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 255/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 256/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 257/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 258/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 259/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 260/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 261/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 262/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 263/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 264/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 265/300\n",
            "13/13 [==============================] - 7s 533ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 266/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 267/300\n",
            "13/13 [==============================] - 7s 539ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 268/300\n",
            "13/13 [==============================] - 7s 534ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 269/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 270/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 271/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 272/300\n",
            "13/13 [==============================] - 7s 538ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 273/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 274/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 275/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 276/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 277/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 278/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 279/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 280/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 281/300\n",
            "13/13 [==============================] - 7s 537ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 282/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 283/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 284/300\n",
            "13/13 [==============================] - 7s 540ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 285/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 286/300\n",
            "13/13 [==============================] - 7s 548ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 287/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 288/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 289/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 290/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 291/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 292/300\n",
            "13/13 [==============================] - 7s 545ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 293/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 294/300\n",
            "13/13 [==============================] - 7s 546ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 295/300\n",
            "13/13 [==============================] - 7s 542ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 296/300\n",
            "13/13 [==============================] - 7s 536ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 297/300\n",
            "13/13 [==============================] - 7s 535ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 298/300\n",
            "13/13 [==============================] - 7s 543ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 299/300\n",
            "13/13 [==============================] - 7s 544ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n",
            "Epoch 300/300\n",
            "13/13 [==============================] - 7s 541ms/step - loss: 1.4525e-08 - val_loss: 4.8093e-09\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXjsbbb-6lwf",
        "outputId": "d16bf1b5-290f-4594-cd83-f619ff11eed9"
      },
      "source": [
        "#Rescaling the data \r\n",
        "import sklearn.metrics as metrics\r\n",
        "yhat = model.predict(test_x)\r\n",
        "\r\n",
        "inv_yhat = scale_y.inverse_transform(yhat)\r\n",
        "\r\n",
        "inv_y = scale_y.inverse_transform(test_y)\r\n",
        "\r\n",
        "#Finding the accuracy with RMSE function\r\n",
        "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\r\n",
        "print('Test RMSE: %.3f' % rmse)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.144\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}